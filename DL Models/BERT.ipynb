{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, mode, tokenizer, path):\n",
    "        assert mode in ['train', 'val']\n",
    "        self.mode = mode\n",
    "        self.df = pd.read_csv(path + mode + '.tsv', sep='\\t').fillna('')\n",
    "        self.len = len(self.df)\n",
    "        self.tokenizer = tokenizer # BERT tokenizer\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        statement, label = self.df.iloc[idx, :].values\n",
    "        label_tensor = torch.tensor(label)\n",
    "            \n",
    "        word_pieces = ['[CLS]']\n",
    "        statement = self.tokenizer.tokenize(statement)\n",
    "        word_pieces += statement + ['[SEP]']\n",
    "        len_st = len(word_pieces)\n",
    "            \n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "            \n",
    "        segments_tensor = torch.tensor([0] * len_st, dtype=torch.long)\n",
    "            \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reforming the dataset to fit the model\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "        \n",
    "    # zero padding\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, batch_first=True)\n",
    "    \n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0,1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from IPython.display import display, clear_output\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "NUM_LABELS = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, num_labels=NUM_LABELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = FakeNewsDataset('train', tokenizer=tokenizer, path='./')\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tuning of BERT\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    \n",
    "    loop = tqdm(trainloader)\n",
    "    for batch_idx, data in enumerate(loop):\n",
    "        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids=tokens_tensors,\n",
    "                      token_type_ids=segments_tensors,\n",
    "                      attention_mask=masks_tensors,\n",
    "                      labels=labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        logits = outputs[1]\n",
    "        _, pred = torch.max(logits.data, 1)\n",
    "        train_acc = accuracy_score(pred.cpu().tolist(), labels.cpu().tolist())\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        loop.set_description(f'Epoch [{epoch + 1}/{NUM_EPOCHS}]')\n",
    "        loop.set_postfix(acc=train_acc, loss=train_loss/(batch_idx+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'BERT/best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valset_fnn = FakeNewsDataset('val', tokenizer=tokenizer, path='./')\n",
    "print('fnn valset size:', valset_fnn.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valloader_fnn = DataLoader(valset_fnn, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "def evaluate(valloader):\n",
    "    true = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data in valloader:\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(device) for t in data if t is not None]\n",
    "\n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            val_outputs = model(input_ids=tokens_tensors,\n",
    "                               token_type_ids=segments_tensors,\n",
    "                               attention_mask=masks_tensors)\n",
    "\n",
    "            logits = val_outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "\n",
    "            labels = data[3]\n",
    "            true.extend(labels.cpu().tolist())\n",
    "            predictions.extend(pred.cpu().tolist())\n",
    "\n",
    "    cm = confusion_matrix(true, predictions, labels=[1, 0], normalize='pred')\n",
    "    print(cm)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Real', 'Fake'])\n",
    "    disp.plot()\n",
    "\n",
    "    accuracy = accuracy_score(true, predictions)\n",
    "    precision = precision_score(true, predictions)\n",
    "    recall = recall_score(true, predictions)\n",
    "    f1 = f1_score(true, predictions)\n",
    "    \n",
    "    print('\\nAccuracy:', accuracy)\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "    print('F1 Score:', f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fake News Net:')\n",
    "print('Confusion Matrix:')\n",
    "evaluate(valloader_fnn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
